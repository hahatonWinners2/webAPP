import asyncio
from fastapi import APIRouter, UploadFile, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
import json
import tempfile
from pydantic import BaseModel
from copy import deepcopy
from random import uniform
import urllib
from bs4 import BeautifulSoup
import httpx

from utils.algorithm.processor import get_answers
from utils.claim_generation.main import generate_pdf
from schemas.client import ClientCreate

from app.clients.router import create_client
from transformers import pipeline


class CourtData(BaseModel):
    court_name: str
    court_address: str
    istec: str
    istec_inn: str
    istec_ogrn: str
    istec_address: str
    otvetchik_name: str
    otvetchik_address: str
    damage_sum: str
    consumption_period: str
    activity_type: str
    act_date: str
    expertise_date: str
    tariff_calculation: str


router = APIRouter()


@router.post("/upload-json/")
async def upload_json(file: UploadFile, background_tasks: BackgroundTasks):
    if file.content_type != "application/json":
        raise HTTPException(status_code=400, detail="Invalid file type. JSON required.")
    content = await file.read()
    try:
        data = json.loads(content)
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON file.")
    
    answers = get_answers(deepcopy(data))

    info = []

    for i, d in enumerate(data):
        info.append({'accountId': d['accountId'], 'isCommercial': answers[i], 'address': d['address']})
        d['suspicion'] = answers[i]
        new_cl = ClientCreate.model_validate(d)
        background_tasks.add_task(create_client, new_cl)

    return info


@router.post("/get_claim")
async def create_pdf(data: CourtData):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        generate_pdf(data.model_dump(), tmp_file.name)
        tmp_file_path = tmp_file.name

    return FileResponse(
        path=tmp_file_path,
        media_type='application/pdf',
        filename="court_data.pdf",
        background=None
    )


classifier = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli",
    device=-1
)

HIGH_CONFIDENCE = 0.60
MEDIUM_CONFIDENCE = 0.40

candidate_labels = ["Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğµ ÑƒÑĞ»ÑƒĞ³Ğ¸", "Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ", "Ğ½Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¾"]

keywords = [
    "Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¼Ğ°ÑÑ‚ĞµÑ€",
    "ÑƒÑĞ»ÑƒĞ³Ğ¸ Ğ½Ğ° Ğ´Ğ¾Ğ¼Ñƒ",
    "Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ±ĞµĞ· Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ğ¸",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚",
    "Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ°ÑÑ‚ĞµÑ€",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°",
    "Ğ²Ñ‹ĞµĞ·Ğ´ Ğ½Ğ° Ğ´Ğ¾Ğ¼",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¼Ğ¾Ğ½Ñ‚",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ğ°Ñ ÑƒĞ±Ğ¾Ñ€ĞºĞ°",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ ÑĞ°Ğ½Ñ‚ĞµÑ…Ğ½Ğ¸Ğº",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¸Ğº",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ¸ĞºĞ¼Ğ°Ñ…ĞµÑ€",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ğ°Ñ Ğ½ÑĞ½Ñ",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¿ĞµÑ‚Ğ¸Ñ‚Ğ¾Ñ€",
    "ÑƒÑĞ»ÑƒĞ³Ğ¸ Ğ±ĞµĞ· Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ğ¸",
    "Ğ½ĞµĞ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒ",
    "ÑĞ°Ğ¼Ğ¾Ğ·Ğ°Ğ½ÑÑ‚Ñ‹Ğ¹",
    "Ñ„Ñ€Ğ¸Ğ»Ğ°Ğ½ÑĞµÑ€",
    "Ğ½ĞµĞ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»ÑƒĞ³Ğ¸",
    "Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ±ĞµĞ· Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸",
    "Ğ½ĞµĞ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ",
    "ÑƒÑĞ»ÑƒĞ³Ğ¸ Ğ±ĞµĞ· Ğ˜ĞŸ",
    "Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ğ¹ Ğ¼Ğ°ÑÑ‚ĞµÑ€",
    "ÑƒÑĞ»ÑƒĞ³Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼",
    "Ñ‡Ğ°ÑÑ‚Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ",
    "Ğ±Ñ‹Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑƒÑĞ»ÑƒĞ³Ğ¸ Ğ½Ğ° Ğ´Ğ¾Ğ¼Ñƒ",
    "Ğ½ĞµĞ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ°ÑÑ‚ĞµÑ€"
]
legal_keywords = [
    "Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚",
    "ĞĞĞ",
    "Ğ˜ĞŸ",
    "Ğ˜ĞĞ",
    "Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ",
    "Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ°",
    "Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ",
    "Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ",
    "ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¾ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸",
    "Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¸Ğ½ÑĞ¿ĞµĞºÑ†Ğ¸Ñ",
    "Ğ¿Ğ°Ñ‚ĞµĞ½Ñ‚",
    "ÑƒÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹",
    "ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ»Ğ¸Ñ†Ğ¾",
    "Ñ„Ğ¸Ñ€Ğ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
    "ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ¼Ñ",
    "ĞĞ“Ğ Ğ",
    "Ğ²Ñ‹Ğ¿Ğ¸ÑĞºĞ° Ğ¸Ğ· Ğ•Ğ“Ğ Ğ®Ğ›",
    "Ğ²Ñ‹Ğ¿Ğ¸ÑĞºĞ° Ğ¸Ğ· Ğ•Ğ“Ğ Ğ˜ĞŸ",
    "Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼",
    "Ğ±ÑƒÑ…Ğ³Ğ°Ğ»Ñ‚ĞµÑ€ÑĞºĞ°Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ",
    "ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ",
    "Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ",
    "ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ²",
    "Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ",
    "Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ¾ÑˆĞ»Ğ¸Ğ½Ğ°",
    "Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹",
    "Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ",
    "Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ñ‹",
    "Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞµÑÑ‚Ñ€ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ğ¹",
    "ÑƒÑÑ‚Ğ°Ğ² Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
    "ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ´Ñ€ĞµÑ",
    "Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ",
    "Ğ°ĞºĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ",
    "Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
    "ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°",
    "Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ"
]
excluded_domains = ["avito.ru", "2gis.ru"]

async def extract_real_url(ddg_link: str) -> str:
    parsed = urllib.parse.urlparse(ddg_link)
    params = urllib.parse.parse_qs(parsed.query)
    return urllib.parse.unquote(params.get('uddg', [''])[0])


async def search_address(address: str) -> list:
    query = f"{address} ÑƒÑĞ»ÑƒĞ³Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ñ"
    url = f"https://html.duckduckgo.com/html/?q={urllib.parse.quote(query)}&kl=ru-ru"

    headers = {
        'User-Agent': 'Mozilla/5.0',
        'Referer': 'https://duckduckgo.com/'
    }
    results = []
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=headers, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            for link in soup.select('.result__a'):
                ddg_url = link.get('href', '')
                real_url = await extract_real_url(ddg_url)
                if real_url:
                    results.append({
                        'title': link.text.strip(),
                        'url': real_url
                    })
        return results[:2]
    except Exception as e:
        print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°: {e}")
        return []

async def zero_shot_classify(text: str) -> tuple[str, float]:
    try:
        result = classifier(text, candidate_labels)
        top_label = result['labels'][0]
        score = result['scores'][0]
        return top_label, score
    except Exception as e:
        print(f"ĞÑˆĞ¸Ğ±ĞºĞ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸: {e}")
        return "Ğ½Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¾", 0.0

async def analyze_site(url: str) -> tuple:
    domain = urllib.parse.urlparse(url).netloc.lower()
    if any(excluded_domain in domain for excluded_domain in excluded_domains):
        return "âšª Ğ˜ÑĞºĞ»ÑÑ‡ĞµĞ½", []

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
    }

    try:
        if not url.startswith('http'):
            url = f'http://{url}'
        async with httpx.AsyncClient() as client:
            resp = await client.get(url, headers=headers, timeout=10)

    except Exception as e:
        return f"ğŸ”´ ĞĞ¨Ğ˜Ğ‘ĞšĞ: {str(e)}", []

    soup = BeautifulSoup(resp.text, 'html.parser')
    text = soup.get_text(separator=' ', strip=True).lower()
    metas = ' '.join([meta.get('content', '') for meta in soup.find_all('meta')]).lower()
    titles = soup.title.string if soup.title else ''
    full_text = ' '.join([text, metas, titles.lower()])

    # Keyword checks
    found_keywords = [kw for kw in keywords if kw in full_text]
    found_legal = [kw for kw in legal_keywords if kw in full_text]

    # Zero-shot classification
    label, score = await zero_shot_classify(full_text[:1000])
    label_upper = label.upper()

    if score >= HIGH_CONFIDENCE:
        emoji = "ğŸŸ¢"
    elif score >= MEDIUM_CONFIDENCE:
        emoji = "ğŸŸ¡"
    else:
        emoji = "ğŸ”´"

    result_string = f"{emoji} {label_upper} ({score:.2f})"

    if found_keywords and not found_legal:
        return f"ğŸ”´ ĞŸĞĞ”ĞĞ—Ğ Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ™ | {result_string}", found_keywords
    elif found_legal:
        return f"ğŸŸ¢ ĞĞĞ ĞœĞĞ›Ğ¬ĞĞ«Ğ™ | {result_string}", found_legal
    else:
        return f"âšª ĞĞ• ĞĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ | {result_string}", []


@router.get("/analyze_address")
async def analyze_address_endpoint(address: str):
    if not address:
        raise HTTPException(status_code=400, detail="Address parameter is required")

    search_results = await search_address(address)

    analyzed_sites = []
    for site in search_results:
        status, found_keywords = await analyze_site(site['url'])
        analyzed_sites.append({
            "title": site['title'],
            "url": site['url'],
            "status": status,
            "found_keywords": found_keywords
        })
        asyncio.sleep(uniform(0.5, 1.5))

    return {"address": address, "analyzed_sites": analyzed_sites}

